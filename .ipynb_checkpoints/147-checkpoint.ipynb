{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2dbba1-fdb0-4b9a-b973-fa35236614c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = '05-07'\n",
    "N_SPLITS = 5\n",
    "SEED = 1\n",
    "CLIP_UPPER_RATE = 1.5\n",
    "CLIP_LOWER_RATE = 1.0\n",
    "Y_THRESHOLD = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5f0dd5-a246-4526-9f6f-b7c230f63c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import configparser\n",
    "from functools import partial\n",
    "from itertools import chain\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "SINCE = time.time()\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error as mse, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearnex import patch_sklearn\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "INI_FILEPATH = os.path.join(os.path.expanduser('~'), 'aiquest2021-assesment', 'config.ini')\n",
    "config.read(INI_FILEPATH)\n",
    "if config['FOLDER']['SCRIPTS'] not in sys.path:\n",
    "    sys.path.append(config['FOLDER']['SCRIPTS'])\n",
    "from logging_util import get_logger, timer\n",
    "from feature_engineering import make_or_load_features\n",
    "from text_feature_extraction import make_or_load_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7125c616-a48d-40f6-81a1-d4c6f0cb086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn高速化\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead24866-4ab0-4bf9-a089-983b33962e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join(config['FOLDER']['EXPERIMENTS'], EXPERIMENT)\n",
    "os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a170ad-261d-40f3-9ec9-9982b95de6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger(__name__, os.path.join(output_dir, 'log.log'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb4b37e-8ad3-4869-9570-658fe96b49f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer('Load dataset', logger):\n",
    "    train = pd.read_csv(os.path.join(config['FOLDER']['INPUT'], 'train.csv'))\n",
    "    test = pd.read_csv(os.path.join(config['FOLDER']['INPUT'], 'test.csv'))\n",
    "    sample_submit = pd.read_csv(os.path.join(config['FOLDER']['INPUT'], 'sample_submit.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee7445e-2ebb-4bb5-87e9-7fae2f230689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020969c7-5a4c-4563-a9d1-ae72d3bf2cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(config['FOLDER']['FEATURES'], 'train_features.csv')\n",
    "test_path = os.path.join(config['FOLDER']['FEATURES'], 'test_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ff50fe-043e-4b05-b096-c620a9f1b94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_test = make_or_load_features(train, test, train_path, test_path, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8046ea-5084-464c-9cb8-1f7d9f4830d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns = [re.sub(r\"[:;/']\", '', c) for c in X.columns]\n",
    "X_test.columns = [re.sub(r\"[:;/']\", '', c) for c in X.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9695b0ea-4e86-49e4-b1f8-efe9db624c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering\n",
    "area_features = ['latitude', 'longitude']\n",
    "\n",
    "# Onehot encoding\n",
    "categorical_features = ['cancellation_policy', 'bed_type', 'city', 'neighbourhood', 'property_type',\n",
    "                        'room_type', 'zipcode5', 'zipcode_1st_digit']\n",
    "\n",
    "# They do not need to be encoded\n",
    "int_flag_features = ['cleaning_fee', 'host_has_profile_pic', 'host_identity_verified',\n",
    "                     'instant_bookable', 'has_thumbnail', 'zipcode_imputed']\n",
    "\n",
    "# Already one-hot style\n",
    "amenity_onehot_features = [c for c in X.columns if c.startswith('has_') and c.endswith('_amenity')]\n",
    "\n",
    "discrete_features = categorical_features + int_flag_features + amenity_onehot_features\n",
    "\n",
    "# Scaling, transformation\n",
    "continuous_features = [\n",
    "    c for c in X.columns\n",
    "    if c not in discrete_features + area_features + ['id', 'y']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019ea6c8-47e4-44ad-b3c1-ead7ac3338ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Reduce dimension & whitening\n",
    "## training\n",
    "decomposer = PCA(n_components=0.8, random_state=SEED, whiten=True).fit(X[amenity_onehot_features])\n",
    "X_amenity_components = decomposer.transform(X[amenity_onehot_features])\n",
    "amenity_components_columns = [f'amenity_x{i + 1}' for i in range(decomposer.n_components_)]\n",
    "X_amenity_components = pd.DataFrame(data=X_amenity_components,\n",
    "                                    columns=amenity_components_columns)\n",
    "X = pd.concat([X, X_amenity_components], axis=1)\n",
    "# X.drop(columns=amenity_onehot_features, inplace=True)\n",
    "X_test_amenity_components = decomposer.transform(X_test[amenity_onehot_features])\n",
    "X_test_amenity_components = pd.DataFrame(data=X_test_amenity_components,\n",
    "                                         columns=amenity_components_columns)\n",
    "X_test = pd.concat([X_test, X_test_amenity_components], axis=1)\n",
    "# X_test.drop(columns=amenity_onehot_features, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f95431-2772-402c-bc43-41ad100cd640",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda s1, s2: ' '.join([s1, s2])\n",
    "train['name+description'] = np.vectorize(f)(train['name'], train['description'])\n",
    "test['name+description'] = np.vectorize(f)(test['name'], test['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b076d75-ebeb-467c-ad3d-c04f9abfe06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vec_train, vec_test = make_or_load_vector(train=train,\n",
    "                                          test=test,\n",
    "                                          feature_dir=config['FOLDER']['FEATURES'],\n",
    "                                          logger=logger,\n",
    "                                          text_column='name+description',\n",
    "                                          embedder='lda_count',\n",
    "                                          overwrite=True,\n",
    "                                          random_state=85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b168194-700c-4129-9117-c28721a18ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vec_columns = vec_train.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94030f4-0168-4cfe-b270-7910316c6981",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X = pd.concat([X, vec_train], axis=1)\n",
    "X_test = pd.concat([X_test, vec_test], axis=1)\n",
    "X.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a55e231-3d17-4b4c-8549-137f4d81c2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "passthrough_features = amenity_onehot_features \\\n",
    "                     + amenity_components_columns \\\n",
    "                     + int_flag_features \\\n",
    "                     + area_features \\\n",
    "                     + vec_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb203dc-190f-4ca3-ac46-087c2cb94114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifier(continuous_features, discrete_features, passthrough_features, random_state):\n",
    "    continuous_preprocessor = Pipeline(\n",
    "        steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median'))\n",
    "        ]\n",
    "    )\n",
    "    categorical_preprocessor = OneHotEncoder(handle_unknown='ignore')\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('categorical', categorical_preprocessor, continuous_features),\n",
    "            ('continuous', continuous_preprocessor, discrete_features),\n",
    "            ('others', 'passthrough', passthrough_features)\n",
    "        ],\n",
    "        sparse_threshold=0\n",
    "    )\n",
    "    return Pipeline(\n",
    "        steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', ComplementNB())\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a24773-bb90-4c7e-affe-ca81c8ef1eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d9ca49-ae5d-41c2-a6b2-3cac274ca07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_min, y_max = train['y'].min(), train['y'].max()  # clipping に必要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4635f0-0076-41cf-9132-c02a754b5bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ターゲットは対数変換する\n",
    "y = train.set_index('id').loc[X['id']]['y']\n",
    "y_log = np.log(y)\n",
    "y_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f76408-6b34-4a93-ae64-7b18e5d5718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified K-fold 用\n",
    "y_labels = pd.cut(y_log, bins=3, labels=range(3))\n",
    "y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053966fb-02a2-43db-b6ab-14335f5a58fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_threshold = np.log(Y_THRESHOLD)\n",
    "y_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb51038-ca39-4125-bd6f-8cfee006bcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_test = X_test['id'].values\n",
    "id_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05880a81-6325-471e-a90e-5dd836f5fa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a659e3-de7c-48d3-a815-99b87c0841e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@np.vectorize\n",
    "def choice_prediction(pred1: float, pred2: float, pred3: float) -> float:\n",
    "    \"\"\"分類器の予測結果に従いモデル②③のどちらの予測を採用するかを決める\n",
    "\n",
    "    pred1: モデル①の予測結果（確率とかではなくクラス）\n",
    "    pred2: モデル②の予測結果\n",
    "    pred3: モデル③の予測結果\n",
    "    \n",
    "    https://github.com/Quvotha/aiquest2021-assesment/issues/8#issuecomment-892219128\n",
    "    \"\"\"\n",
    "    return pred2 if pred1 == 1 else pred3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b015b50d-ebfc-4fe9-8596-526a586a7fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "for i, (train_idx, vaild_idx) in enumerate(splitter.split(X=X, y=y_labels)):\n",
    "    num_fold = i + 1\n",
    "    logger.debug('Start fold {} ({:.3f} seconds passed)'.format(num_fold, time.time() - SINCE))\n",
    "\n",
    "    # 訓練データと検証データに分割\n",
    "    id_train = X.iloc[train_idx]['id'].values\n",
    "    X_train = X.iloc[train_idx].drop(columns=['id'])\n",
    "    y_train = y_log[train_idx].values\n",
    "    id_valid = X.iloc[vaild_idx]['id'].values\n",
    "    X_valid = X.iloc[vaild_idx].drop(columns=['id'])\n",
    "    y_valid = y_log[vaild_idx].values\n",
    "    \n",
    "    # モデルの訓練\n",
    "    ## 分類モデルの訓練\n",
    "    with timer('Training: classifier', logger):\n",
    "        y_train_clf = 1 * (y_train > y_threshold)\n",
    "        y_valid_clf = 1 * (y_valid > y_threshold)\n",
    "        # モデル①\n",
    "        class_weights = compute_class_weight(class_weight='balanced',\n",
    "                                             classes=np.array([0, 1]),\n",
    "                                             y=y_train_clf)\n",
    "        classifier = CatBoostClassifier(n_estimators=300,\n",
    "                                        cat_features=discrete_features,\n",
    "                                        class_weights=class_weights,\n",
    "                                        random_state=SEED)\n",
    "        classifier.fit(X_train, y_train_clf)\n",
    "    ## 分類モデルの評価\n",
    "    with timer('Evaluate classifier', logger):\n",
    "        pred_train_clf = classifier.predict(X_train)\n",
    "        logger.debug('Training f1 score: {:.6f}'.format(f1_score(y_train_clf, pred_train_clf)))\n",
    "        logger.debug('Training precision: {:.6f}'.format(precision_score(y_train_clf, pred_train_clf)))\n",
    "        logger.debug('Training recall: {:.6f}'.format(recall_score(y_train_clf, pred_train_clf)))\n",
    "        pred_valid_clf = classifier.predict(X_valid)\n",
    "        logger.debug('Validation f1 score: {:.6f}'.format(f1_score(y_valid_clf, pred_valid_clf)))\n",
    "        logger.debug('Validation precision: {:.6f}'.format(precision_score(y_valid_clf, pred_valid_clf)))\n",
    "        logger.debug('Validation recall: {:.6f}'.format(recall_score(y_valid_clf, pred_valid_clf)))\n",
    "    ## `y` 予測モデルの訓練\n",
    "    with timer('Training regressor', logger):\n",
    "        class1_mask = y_train > y_threshold\n",
    "        # モデル③\n",
    "        estimator0 = LGBMRegressor(n_estimators=300,\n",
    "                                   random_state=SEED,\n",
    "                                   n_jobs=-1,\n",
    "                                   learning_rate=0.1,\n",
    "                                   importance_type='gain')\n",
    "        estimator0.fit(X_train[~class1_mask], y_train[~class1_mask], categorical_feature=discrete_features)\n",
    "        # モデル②\n",
    "        estimator1 = LGBMRegressor(n_estimators=300,\n",
    "                                   random_state=SEED,\n",
    "                                   n_jobs=-1,\n",
    "                                   learning_rate=0.1,\n",
    "                                   importance_type='gain')\n",
    "        estimator1.fit(X_train[class1_mask], y_train[class1_mask], categorical_feature=discrete_features)\n",
    "        \n",
    "    # 予測結果を保存する\n",
    "    with timer('Prediction', logger):\n",
    "        # 訓練データ\n",
    "        proba_train = classifier.predict_proba(X_train)\n",
    "        pred_train0 = estimator0.predict(X_train)\n",
    "        pred_train1 = estimator1.predict(X_train)\n",
    "        pred_train = choice_prediction(pred1=pred_train_clf, pred2=pred_train1, pred3=pred_train0)\n",
    "        pred_train = pd.DataFrame(data=pred_train, columns=['pred'])\n",
    "        pred_train['pred'] = np.exp(pred_train['pred'])\n",
    "        pred_train['pred'].clip(lower=y_min * CLIP_LOWER_RATE, upper=y_max * CLIP_UPPER_RATE, inplace=True)\n",
    "        # 検証データ\n",
    "        proba_valid = classifier.predict_proba(X_valid)\n",
    "        pred_valid0 = estimator0.predict(X_valid)\n",
    "        pred_valid1 = estimator1.predict(X_valid)\n",
    "        pred_valid = choice_prediction(pred1=pred_valid_clf, pred2=pred_valid1, pred3=pred_valid0)\n",
    "        pred_valid = pd.DataFrame(data=pred_valid, columns=['pred'])\n",
    "        pred_valid['pred'] = np.exp(pred_valid['pred'])\n",
    "        pred_valid['pred'].clip(lower=y_min * CLIP_LOWER_RATE, upper=y_max * CLIP_UPPER_RATE, inplace=True)\n",
    "        # テストデータ\n",
    "        proba_test = classifier.predict_proba(X_test.drop(columns=['id']))\n",
    "        pred_test_clf = classifier.predict(X_test.drop(columns=['id']))\n",
    "        pred_test0 = estimator0.predict(X_test.drop(columns=['id']))\n",
    "        pred_test1 = estimator1.predict(X_test.drop(columns=['id']))\n",
    "        pred_test = choice_prediction(pred1=pred_test_clf, pred2=pred_test1, pred3=pred_test0)\n",
    "        pred_test = pd.DataFrame(data=pred_test, columns=['pred'])\n",
    "        pred_test['pred'] = np.exp(pred_test['pred'])\n",
    "        pred_test['pred'].clip(lower=y_min * CLIP_LOWER_RATE, upper=y_max * CLIP_UPPER_RATE, inplace=True)\n",
    "    with timer('Save prediction', logger):\n",
    "        ## 訓練データ\n",
    "        pred_train['id'] = id_train\n",
    "        pred_train.to_csv(os.path.join(output_dir, f'cv_fold{num_fold}_training.csv'), index=False)\n",
    "        ## 検証データ\n",
    "        pred_valid['id'] = id_valid\n",
    "        pred_valid.to_csv(os.path.join(output_dir, f'cv_fold{num_fold}_validation.csv'), index=False)\n",
    "        ## テストデータ\n",
    "        pred_test['id'] = id_test\n",
    "        pred_test.to_csv(os.path.join(output_dir, f'cv_fold{num_fold}_test.csv'), index=False)\n",
    "    ## モデルの保存\n",
    "    with timer('Save model', logger):\n",
    "        filepath_fold_model = os.path.join(output_dir, f'cv_fold{num_fold}_model.pkl')\n",
    "        with open(filepath_fold_model, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'class0': estimator0,\n",
    "                'class1': estimator1,\n",
    "                'classifier': classifier\n",
    "            }, f)\n",
    "    logger.debug('Complete fold {} ({:.3f} seconds passed)'.format(num_fold, time.time() - SINCE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829a41aa-0ab9-4c86-91ab-c8da3a1e9306",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351e8157-88d0-4c2d-87d8-49c9fc75ad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = partial(mse, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08b755e-12d9-4927-a65d-18dccccc48d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67abfc53-d0bf-4c45-aa7b-60b0e409dfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_dfs = []\n",
    "for i in range(N_SPLITS):\n",
    "    num_fold = i + 1\n",
    "    # Read cv result\n",
    "    pred_df = pd.read_csv(os.path.join(output_dir, f'cv_fold{num_fold}_training.csv'))\n",
    "    pred_df['actual'] = train.loc[pred_df['id'], 'y'].values\n",
    "    cv_loss = rmse(pred_df['actual'], pred_df['pred'])\n",
    "    logger.info('CV fold {} training loss={:.7f}'.format(num_fold, cv_loss))\n",
    "    metrics['train_losses'].append(cv_loss)\n",
    "    pred_train_dfs.append(pred_df)\n",
    "\n",
    "metrics['train_losses_avg'] = np.mean(metrics['train_losses'])\n",
    "metrics['train_losses_std'] = np.std(metrics['train_losses'])\n",
    "\n",
    "logger.info('CV training loss: average={:.7f}, std={:.7f}' \\\n",
    "            .format(metrics['train_losses_avg'], metrics['train_losses_std']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183e44ba-fcae-4263-97bf-13783ea6f4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = pd.concat(pred_train_dfs).groupby('id').sum()\n",
    "pred_train = pred_train / N_SPLITS\n",
    "pred_train['actual'] = train.loc[pred_train.index, 'y'].values\n",
    "pred_train.to_csv(os.path.join(output_dir, 'prediction_train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90816dd-d8c6-4450-aaa4-f9a455020df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = rmse(pred_train['actual'], pred_train['pred'])\n",
    "metrics['train_loss'] = train_loss\n",
    "logger.info('Training loss: {:.7f}'.format(train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4489ea2-9a20-42ff-ba60-2b7abb49be37",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_valid_dfs = []\n",
    "for i in range(N_SPLITS):\n",
    "    num_fold = i + 1\n",
    "    # Read cv result\n",
    "    pred_df = pd.read_csv(os.path.join(output_dir, f'cv_fold{num_fold}_validation.csv'))\n",
    "    pred_df['actual'] = train.loc[pred_df['id'], 'y'].values\n",
    "    cv_loss = rmse(pred_df['actual'], pred_df['pred'])\n",
    "    logger.info('CV fold {} validation loss={:.7f}'.format(num_fold, cv_loss))\n",
    "    metrics['valid_losses'].append(cv_loss)\n",
    "    pred_valid_dfs.append(pred_df)\n",
    "\n",
    "metrics['valid_losses_avg'] = np.mean(metrics['valid_losses'])\n",
    "metrics['valid_losses_std'] = np.std(metrics['valid_losses'])\n",
    "\n",
    "logger.info('CV validation loss: average={:.7f}, std={:.7f}' \\\n",
    "            .format(metrics['valid_losses_avg'], metrics['valid_losses_std']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a523d0-6d0f-4c71-b83a-9fb7bdc25ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_valid = pd.concat(pred_valid_dfs).groupby('id').sum()\n",
    "pred_valid = pred_valid / N_SPLITS\n",
    "pred_valid['actual'] = train.loc[pred_valid.index, 'y'].values\n",
    "pred_valid.to_csv(os.path.join(output_dir, 'prediction_valid.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cf61a0-f46e-4ae6-ac2b-57886a38b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loss = rmse(pred_valid['actual'], pred_valid['pred'])\n",
    "metrics['valid_loss'] = valid_loss\n",
    "logger.info('Validation loss: {:.7f}'.format(valid_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37de6d29-3393-4601-bd09-4d8e90e7203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(output_dir, 'metrics.json'), 'w') as f:\n",
    "    json.dump(metrics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74837457-1c59-4fde-8199-68ed7eb47825",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12.5, 6.5))\n",
    "plt.suptitle('Actual vs Prediction')\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "ax = sns.scatterplot(pred_train['actual'], pred_train['pred'])\n",
    "plt.plot(pred_train['actual'], pred_train['actual'], color='black', linewidth=0.5)\n",
    "ax.set_xlim(0, 2000)\n",
    "ax.set_ylim(0, 2000)\n",
    "ax.set_title('Training set');\n",
    "# plt.axes().set_aspect('equal')\n",
    "plt.subplot(1, 2, 2)\n",
    "ax = sns.scatterplot(pred_valid['actual'], pred_valid['pred'])\n",
    "plt.plot(pred_valid['actual'], pred_valid['actual'], color='black', linewidth=0.5)\n",
    "ax.set_xlim(0, 2000)\n",
    "ax.set_ylim(0, 2000)\n",
    "plt.title('Validation set');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d3c01d-1274-4dd9-8937-9c6a7b187cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metrics['train_losses'], label='training set')\n",
    "plt.plot(metrics['valid_losses'], label='validation set')\n",
    "plt.title('Training/Validation loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e4f178-f8d9-46f8-9945-1fc9e081ef6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd34fb24-8354-4580-a7ce-0cf6c9103233",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_dfs = [pd.read_csv(os.path.join(output_dir, f'cv_fold{i + 1}_test.csv')) for i in range(N_SPLITS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2db888-5d56-4c81-bc84-ddc0dbcff11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = pd.concat(pred_test_dfs).groupby('id').sum()\n",
    "pred_test = pred_test / N_SPLITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192b04c4-d040-44c6-a745-dc69b72f2a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test.to_csv(os.path.join(output_dir, f'{EXPERIMENT}_submission.csv'), header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7563819b-0821-4546-bb19-72b9fe675283",
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim_min, ylim_max = np.log(y_min * CLIP_LOWER_RATE), np.log(y_max * CLIP_UPPER_RATE)\n",
    "fig = plt.figure(figsize=(10.5, 10.5))\n",
    "plt.subplot(2, 2, 1)\n",
    "ax = sns.histplot(y_log)\n",
    "ax.set_title('Actual y, log-scaled')\n",
    "ax.set_xlim(ylim_min, ylim_max)\n",
    "sns.despine()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "ax = sns.histplot(np.log(pred_train['pred']))\n",
    "ax.set_title('Training set, log-scaled')\n",
    "ax.set_xlim(ylim_min, ylim_max)\n",
    "sns.despine()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "ax = sns.histplot(np.log(pred_valid['pred']))\n",
    "ax.set_title('Validation set, log-scaled')\n",
    "ax.set_xlim(ylim_min, ylim_max)\n",
    "sns.despine()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "ax = sns.histplot(np.log(pred_test['pred']))\n",
    "ax.set_title('Test set, log-scaled')\n",
    "ax.set_xlim(ylim_min, ylim_max)\n",
    "sns.despine()\n",
    "\n",
    "fig.savefig(os.path.join(output_dir, 'figure.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cbc5c5-c95e-4391-afbf-db142db4a3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug('Complete({:.3f} seconds passed)'.format(time.time() - SINCE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9375b8ba-ac5d-4405-8a80-e7474ac99f89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf185f6-034a-45e3-814f-3ff672cd2f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.5f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bab9993-eee4-40fd-8264-1bc628655adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for i, (train_idx, vaild_idx) in enumerate(splitter.split(X=X, y=y_labels)):\n",
    "    num_fold = i + 1\n",
    "\n",
    "    ## モデルの保存\n",
    "    filepath_fold_model = os.path.join(output_dir, f'cv_fold{num_fold}_model.pkl')\n",
    "    with open(filepath_fold_model, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef617db-9a37-47b6-bc01-5f7f797e9f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6.5, 21.5))\n",
    "for i in range(N_SPLITS):\n",
    "    plt.subplot(5, 1, i + 1)\n",
    "    imp_df = pd.DataFrame(data=model['class0'].feature_importances_, columns=['importance'])\n",
    "    imp_df['feature'] = X_train.columns.tolist()\n",
    "    imp_df.sort_values(['importance', 'feature'], ascending=False, inplace=True)\n",
    "    sns.barplot(data=imp_df.head(25), y='feature', x='importance')\n",
    "fig.savefig(os.path.join(output_dir, 'feature_importance0.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1846d6b4-ccf2-42db-b2a0-fb26b9763eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6.5, 21.5))\n",
    "for i in range(N_SPLITS):\n",
    "    plt.subplot(5, 1, i + 1)\n",
    "    imp_df = pd.DataFrame(data=model['class1'].feature_importances_, columns=['importance'])\n",
    "    imp_df['feature'] = X_train.columns.tolist()\n",
    "    imp_df.sort_values(['importance', 'feature'], ascending=False, inplace=True)\n",
    "    sns.barplot(data=imp_df.head(25), y='feature', x='importance')\n",
    "fig.savefig(os.path.join(output_dir, 'feature_importance0.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a906cd06-5074-4c5b-b4ee-ee66c691d103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4be162-4866-420b-a40d-00745c86c564",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_valid = pd.read_csv(os.path.join(output_dir, 'prediction_valid.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a7df50-87cc-4efc-a981-cce19171c000",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_valid['diff'] = pred_valid['pred'] - pred_valid['actual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4646076f-4668-4a7b-8251-d1e206f3085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.axes().set_aspect('equal')\n",
    "ax = sns.scatterplot(data=pred_valid, x='actual', y='pred')\n",
    "ax = sns.lineplot(data=pred_valid, x='actual', y='actual', color='red')\n",
    "fig.savefig(os.path.join(output_dir, 'compare_actual_prediction.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbbdf17-f5c4-488a-b721-68ed8ac5eee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_valid.describe()\n",
    "sns.histplot(data=pred_valid, x='diff')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdee900-05d6-4aa1-8df0-afc6777d5105",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df = pd.merge(X, pred_valid[['id', 'diff']]).set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0864b9-ffb5-44d6-8694-966e2c390479",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import sweetviz\n",
    "report = sweetviz.analyze(diff_df, target_feat='diff', pairwise_analysis='off')\n",
    "report.show_html(os.path.join(output_dir, ('sweetviz_error_report.html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdf19e6-5aac-48a3-b21b-453ade3e72b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 50\n",
    "pd.options.display.max_columns = diff_df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2173e1a8-1015-4bf3-8238-cc587beddab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df.sort_values('diff').head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171ee684-b922-45f2-8030-4f6b842565a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df.sort_values('diff').tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59cad84-2410-4ba1-91ee-f739aa888ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41a3db7-5c27-4932-8cc2-4a1e9b0d3ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df.corr().sort_values('diff')['diff'].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b883583c-495a-4813-af80-933513913a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "estimator = LGBMRegressor(random_state=SEED, n_jobs=-1, importance_type='gain').fit(diff_df.drop(columns=['diff']), diff_df['diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdf9556-0fa9-4a94-9ef6-dec491951b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df = pd.DataFrame(data=estimator.feature_importances_, columns=['importance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78b61bb-2c0e-42a4-b053-28a23cf87fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df['feature'] = estimator.feature_name_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a966159-79cd-4643-b073-44412f0ed2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df.sort_values(['importance', 'feature'], ascending=False, inplace=True)\n",
    "imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e39fa8-5237-4c82-b1d2-417d1270a2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df.to_csv(os.path.join(output_dir, 'diff_feature_importances.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('ukita_main_env': conda)",
   "language": "python",
   "name": "python395jvsc74a57bd0c19bd4dc0949b9af5049c50e337ed6e21b7d6fbec491433b163f7fe7b877fba9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
